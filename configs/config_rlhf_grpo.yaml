## GRPO Configuration for RLHF with Alpaca-7B
## Single GPU setup with LoRA
## Total: 20,000 completions (matched with PPO)

model:
  base_model_name: 'chavinlo/alpaca-native'   # Alpaca-7B SFT checkpoint
  reward_model_path: 'reward_model_out/checkpoint-1800'  # Trained reward model
  cache_dir: 'models_cache'                   # Local cache for model weights
  max_prompt_length: 135                      # Alpaca prompts 99th pct = 135 tokens
  max_length: 512

  # LoRA configuration
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  lora_target_modules:
    - q_proj
    - v_proj
    - k_proj
    - o_proj

# SFT dataset for prompts during PO
dataset:
  dataset_name: 'tatsu-lab/alpaca'
  dataset_split: 'train'
  prompt_field: 'instruction'
  cache_dir: 'dataset/cache'
  num_train_prompts: 5000                     # Training prompts from SFT dataset
  num_samples: 5000                           # Total samples to draw
  sample_seed: 42

training:
  output_dir: 'rlhf_out/grpo'
  log_dir: 'logs/rlhf_grpo'

  # Rollout / batching
  prompts_per_update: 4                       # per_device_train_batch_size
  steps_per_generation: 1                     # gradient steps per generation
  group_size: 4                               # 4 completions per prompt

  # Total training budget
  # prompts = prompts_per_update × num_updates = 4 × 1250 = 5,000 prompts
  # completions = prompts × group_size = 5,000 × 4 = 20,000 completions
  num_updates: 1250

  # Optimizer reuse
  num_iterations: 2                           # inner iters per update (cf. PPO ppo_epochs)
  gradient_accumulation_steps: 2

  # Generation
  max_new_tokens: 256
  do_sample: true
  rollout_temperature: 0.8
  temperature: 0.8
  top_k: 0
  top_p: 0.95

  # Optimization
  learning_rate: 5.0e-6
  lr_scheduler_type: 'cosine'
  warmup_ratio: 0.03

  # GRPO hyperparameters
  beta: 0.005
  cliprange: 0.2
  baseline: 'mean'

  # Stability
  max_grad_norm: 1.0

  # Precision
  fp16: false
  bf16: true
  use_flash_attention: true
  gradient_checkpointing: true

  # Logging / ckpt
  logging_steps: 10
  save_freq: 500
  save_final_only: true

  # Reproducibility
  seed: 42

monitoring:
  log_reward: true
  log_kl: true
  log_response_length: true
  log_entropy: true
  log_samples: true
  num_sample_prompts: 5
  use_tensorboard: true

safety:
  max_kl: 2.0
